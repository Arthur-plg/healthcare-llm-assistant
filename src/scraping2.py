from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.common.keys import Keys
import time
import pandas as pd

def scrape_carenity_comments():
    # --- Charger la liste des m√©dicaments ---
    df_med = pd.read_csv('/Users/arthurpelong/healthcare-llm-assistant/data/raw/medicaments_carenity.csv')
    list_url = df_med.url.to_list()

    # --- Lancer le navigateur ---
    options = webdriver.ChromeOptions()
    # options.add_argument("--headless")  # D√©commente pour du headless
    options.add_argument("--window-size=1920,1080")
    driver = webdriver.Chrome(options=options)

    # --- Aller sur la page de connexion ---
    driver.get("https://www.carenity.com/connexion")
    wait = WebDriverWait(driver, 15)

    # --- Accepter les cookies ---
    try:
        accept_cookies = wait.until(
            EC.element_to_be_clickable((By.ID, "didomi-notice-agree-button"))
        )
        accept_cookies.click()
        print("‚úÖ Cookies accept√©s")
    except Exception:
        print("‚ö†Ô∏è Pas de popup cookies trouv√©e (peut-√™tre d√©j√† accept√©).")

    # --- Remplir email et mot de passe ---
    email_input = wait.until(
        EC.presence_of_element_located((By.ID, "username"))
    )
    password_input = wait.until(
        EC.presence_of_element_located((By.ID, "password"))
    )
    email_input.send_keys("...")    # Cach√© pour github
    password_input.send_keys("...") # Cach√© pour github
    password_input.send_keys(Keys.RETURN)

    print("‚úÖ Connexion r√©ussie (si identifiants corrects)")

    comments_data = []

    # --- Scraper les commentaires ---
    for url in list_url:
        print("Scraping:", url)
        page = 1

        # Convertir l'URL www ‚Üí membre pour la pagination des commentaires
        base_comment_url = url.replace("www.carenity.com", "membre.carenity.com")

        while True:
            url_paged = base_comment_url if page == 1 else f"{base_comment_url}?page={page}#comments"
            print("Page URL:", url_paged)
            driver.get(url_paged)
            time.sleep(0.5)  # Laisser le temps √† la page de charger

            # R√©cup√©rer tous les commentaires
            comments = driver.find_elements(By.CSS_SELECTOR, "div.content-align-left")
            comments_text = [c.text.strip() for c in comments if c.text.strip()]

            # Condition d'arr√™t : pas de commentaires
            if not comments_text:
                print("‚úÖ Fin des pages pour ce m√©dicament.")
                break

            for text in comments_text:
                comments_data.append({"url": url, "comment": text})

            page += 1
            time.sleep(0.2)

    # --- Sauvegarde ---
    df_comments = pd.DataFrame(comments_data)
    df_comments.to_csv("/Users/arthurpelong/healthcare-llm-assistant/data/raw/comments_final.csv", index=False)

    driver.quit()
    print("üíæ Scraping termin√© et CSV sauvegard√©.")
    return df_comments

if __name__ == "__main__":
    scrape_carenity_comments()